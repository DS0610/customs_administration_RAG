{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607e255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoodongseok/Desktop/rag_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3264661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태정의\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    docs: List[str]\n",
    "    summary: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa02ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive 노드 (문서 검색 노드)\n",
    "def retrieve_docs(state: RAGState):\n",
    "    print(\"Retrieving relevant documents...\")\n",
    "    \n",
    "    loader = WebBaseLoader(\"https://ko.wikipedia.org/wiki/대한민국\") # loader 생성\n",
    "    docs = loader.load() # docs에 문서 로드\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) # splitter 정의\n",
    "    chunks = splitter.split_documents(docs) # docs 청킹\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") # 임베딩 모델 정의\n",
    "    vectordb = Chroma.from_documents(chunks, embedding=embeddings) # 청킹된 문서들을 임베딩시켜서 벡터DB에 저장\n",
    "    \n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\":3}) # 유사한 키워드 3개 검색\n",
    "    retrived_docs = retriever.invoke(state[\"question\"]) # 질문\n",
    "    \n",
    "    return {'docs': [d.page_content for d in retrived_docs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72731681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoner 노드 (문서 요약 및 핵심 추론)\n",
    "def reason_over_docs(state: RAGState):\n",
    "    \"\"\"검색된 문서를 요약하고 질문에 필요한 핵심 맥락 정의\"\"\"\n",
    "    print(\"Reasoning about retrieved docs...\")\n",
    "    \n",
    "    llm = ChatOllama(model=\"llama3:8b\", temperature=0)\n",
    "    context = \"\\n\\n\".join(state[\"docs\"])[:2000]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    너는 문서 요약 및 분석 전문가야.\n",
    "    반드시 한국어로 답변해야 해.\n",
    "    아래 문서를 바탕으로 질문에 대한 핵심 내용을 정리해줘.\n",
    "    \n",
    "    [문서 내용]\n",
    "    {context}\n",
    "    \n",
    "    [질문]\n",
    "    {state['question']} \n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\"summary\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0d9c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator 노드 (최종 응답 생성)\n",
    "def generate_final_answer(state: RAGState):\n",
    "    \"\"\"Reasoner가 정리한 내용을 바탕으로 자연어 답변 생성\"\"\"\n",
    "    print(\"Generating final answer...\")\n",
    "    \n",
    "    llm = ChatOllama(model=\"llama3:8b\", temperature=0.3)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    다음은 사용자의 질문과 관련된 핵심 요약 내용이야.\n",
    "    이를 바탕으로 반드시 자연스럽고 명확한 한국어로 답변해줘.\n",
    "    \n",
    "    [요약된 내용]\n",
    "    {state[\"summary\"]}\n",
    "    \n",
    "    [질문]\n",
    "    {state[\"question\"]}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2bb433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langgraph 구성\n",
    "workflow = StateGraph(RAGState)\n",
    "workflow.add_node(\"retriever\", retrieve_docs)\n",
    "workflow.add_node(\"reasoner\", reason_over_docs)\n",
    "workflow.add_node(\"generator\", generate_final_answer)\n",
    "\n",
    "workflow.add_edge(START, \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"reasoner\")\n",
    "workflow.add_edge(\"reasoner\", \"generator\")\n",
    "workflow.add_edge(\"generator\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bf569ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 대한민국의 수도는 어디인가요?\n",
      "Retrieving relevant documents...\n",
      "Reasoning about retrieved docs...\n",
      "Generating final answer...\n",
      "\n",
      "✅ 최종 답변:\n",
      "😊\n",
      "\n",
      "대한민국의 수도는 서울입니다! 🏙️\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"대한민국의 수도는 어디인가요?\"\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    result = app.invoke({\"question\": question})\n",
    "    print(\"\\n✅ 최종 답변:\")\n",
    "    print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636889df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
