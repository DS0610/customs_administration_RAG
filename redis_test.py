import redis
import requests
import numpy as np
from redis.commands.search.query import Query

# 1. Redis ì—°ê²°
r = redis.Redis(host="localhost", port=6379, decode_responses=False)

# 2. Ollama ì„ë² ë”© í•¨ìˆ˜ (ì„ë² ë”© ì „ìš© ëª¨ë¸ ì‚¬ìš©)
def get_embedding(text: str):
    url = "http://localhost:11434/api/embeddings"
    payload = {"model": "nomic-embed-text", "prompt": text}  # âœ… ì„ë² ë”© ëª¨ë¸
    res = requests.post(url, json=payload)
    data = res.json()
    if "embedding" not in data:
        raise ValueError(f"Ollama ì„ë² ë”© ì˜¤ë¥˜ ì‘ë‹µ: {data}")
    return data["embedding"]

# 3. Ollama ìƒì„± í•¨ìˆ˜ (ë‹µë³€ ìƒì„±)
def generate_answer(prompt: str, context: str = ""):
    url = "http://localhost:11434/api/generate"
    full_prompt = f"ì§ˆë¬¸: {prompt}\n\nì°¸ê³  ë¬¸ì„œ: {context}\n\në‹µë³€:"
    payload = {"model": "llama3.2:3b", "prompt": full_prompt, "stream": False}
    res = requests.post(url, json=payload)
    data = res.json()
    return data.get("response", "âš ï¸ ëª¨ë¸ ì‘ë‹µ ì˜¤ë¥˜")

# 4. ì„ë² ë”© ì°¨ì› ìë™ í™•ì¸
test_emb = get_embedding("ì°¨ì› í™•ì¸ í…ŒìŠ¤íŠ¸")
dim = len(test_emb)
print(f"âœ… ì„ë² ë”© ì°¨ì›: {dim}")

# 5. ì¸ë±ìŠ¤ ìƒì„± (ê¸°ì¡´ ìˆìœ¼ë©´ ì‚­ì œ)
index_name = "qa_index"
try:
    r.ft(index_name).dropindex(delete_documents=True)
except:
    pass

r.ft(index_name).create_index(
    fields=[
        redis.commands.search.field.VectorField("embedding", "FLAT", {
            "TYPE": "FLOAT32",
            "DIM": dim,
            "DISTANCE_METRIC": "COSINE"
        }),
        redis.commands.search.field.TextField("value")
    ]
)

# 6. ì˜ˆì‹œ ë°ì´í„° (Key=ì§ˆë¬¸, Value=ë‹µë³€)
qa_data = {
    "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.",
    "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?": "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì›Œì‹±í„´ D.C.ì…ë‹ˆë‹¤.",
    "ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?": "ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ë„ì¿„ì…ë‹ˆë‹¤.",
    "ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?": "ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ë² ì´ì§•ì…ë‹ˆë‹¤.",
    "í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?": "í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ëŠ” íŒŒë¦¬ì…ë‹ˆë‹¤."
}

# 7. Redisì— ë°ì´í„° ì ì¬
for i, (q, a) in enumerate(qa_data.items()):
    emb = get_embedding(q)
    emb_bytes = np.array(emb, dtype=np.float32).tobytes()
    r.hset(f"doc:{i}", mapping={
        "embedding": emb_bytes,
        "value": a
    })

print("âœ… Redisì— ë°ì´í„° ì ì¬ ì™„ë£Œ")

# 8. ì§ˆì˜ â†’ Redis ê²€ìƒ‰ â†’ ë‹µë³€ ë°˜í™˜
query = "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì…ë‹ˆê¹Œ?"
query_emb = get_embedding(query)
query_emb_bytes = np.array(query_emb, dtype=np.float32).tobytes()

q = Query('*=>[KNN 1 @embedding $vec]') \
    .return_fields("value", "__score__") \
    .dialect(2)

res = r.ft(index_name).search(
    q,
    query_params={"vec": query_emb_bytes}
)

if res.docs:
    cached_answer = res.docs[0]["value"]
    print("ğŸ” Redis ê²€ìƒ‰ ê²°ê³¼:", cached_answer)
else:
    print("âš ï¸ Redisì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    cached_answer = None

# 9. ìºì‹œ ë¯¸ìŠ¤ â†’ Ollama ìƒì„± ëª¨ë¸ í˜¸ì¶œ
if cached_answer:
    final_answer = cached_answer
else:
    final_answer = generate_answer(query)

print("ğŸ¤– ìµœì¢… ë‹µë³€:", final_answer)
